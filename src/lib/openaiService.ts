/**
 * OpenAI Service for HopeVisionAI
 * Handles all AI interactions: symptom analysis, precision chat, and report generation
 */

const OPENAI_API_KEY = import.meta.env.VITE_OPENAI_API_KEY;
const OPENAI_MODEL = import.meta.env.VITE_OPENAI_MODEL || 'gpt-4o';

if (!OPENAI_API_KEY) {
  console.warn('‚ö†Ô∏è VITE_OPENAI_API_KEY is not set. AI features will not work.');
}

import { UnifiedMedicalContext } from '../utils/medicalContext';

interface SymptomAnalysisInput {
  textInput?: string;
  voiceTranscript?: string;
  selectedChips?: string[];
  imageUrls?: string[];
  documentUrls?: string[];
  chatAnswers?: string;
  chatMessages?: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>;
  enrichedSymptoms?: {
    initial_text?: string;
    voice_transcriptions?: string;
    chat_answers?: string;
    selected_chips?: string[];
    images_count?: number;
    documents_count?: number;
  };
  patientProfile?: {
    age?: number;
    gender?: string;
    bloodGroup?: string;
    allergies?: string[];
    medicalHistory?: string;
  };
  // NEW: Unified Context Support
  unifiedContext?: UnifiedMedicalContext;
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface DiagnosticHypothesis {
  disease_name: string;
  confidence: number;
  severity: 'low' | 'medium' | 'high';
  keywords: string[];
  explanation: string;
  is_primary?: boolean;
  is_excluded?: boolean;
}

interface AIReportData {
  overall_severity: 'low' | 'medium' | 'high';
  overall_confidence: number;
  summary: string;
  primary_diagnosis: string;
  primary_diagnosis_confidence: number;
  recommendation_action: string;
  recommendation_text: string;
  diagnostic_hypotheses: DiagnosticHypothesis[];
  explainability_data?: any;
}

/**
 * Analyze symptoms and generate initial AI response for precision chat
 */
export async function analyzeSymptoms(input: SymptomAnalysisInput): Promise<string> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key is not configured');
  }

  const systemPrompt = `Tu es un assistant m√©dical IA sp√©cialis√© dans l'analyse de sympt√¥mes. 
Ton r√¥le est de poser des questions pr√©cises pour affiner le diagnostic initial.

R√®gles importantes:
- Pose des questions claires et concises (maximum 2-3 questions √† la fois)
- Utilise un langage m√©dical adapt√© aux patients
- Sois empathique et rassurant
- Ne pose jamais de diagnostic d√©finitif, mais guide vers plus de pr√©cisions
- Commence toujours par accueillir le patient et lui poser les premi√®res questions de pr√©cision

Format de r√©ponse: Questions directes en fran√ßais, sans formatage sp√©cial.`;

  let userPrompt = `Le patient a d√©crit les sympt√¥mes suivants:\n\n`;

  // USE UNIFIED CONTEXT IF AVAILABLE
  if (input.unifiedContext) {
    userPrompt += input.unifiedContext.combined_text_block;
  } else {
    // FALLBACK TO LEGACY LOGIC
    if (input.textInput) {
      userPrompt += `Description textuelle: ${input.textInput}\n\n`;
    }

    if (input.voiceTranscript) {
      userPrompt += `Transcription vocale: ${input.voiceTranscript}\n\n`;
    }

    if (input.selectedChips && input.selectedChips.length > 0) {
      userPrompt += `Pr√©cisions rapides s√©lectionn√©es: ${input.selectedChips.join(', ')}\n\n`;
    }

    if (input.imageUrls && input.imageUrls.length > 0) {
      userPrompt += `Images fournies: ${input.imageUrls.length} image(s)\n\n`;
    }

    if (input.documentUrls && input.documentUrls.length > 0) {
      userPrompt += `Documents fournis: ${input.documentUrls.length} document(s)\n\n`;
    }

    if (input.patientProfile) {
      userPrompt += `\nProfil patient:\n`;
      if (input.patientProfile.age) userPrompt += `- √Çge: ${input.patientProfile.age} ans\n`;
      if (input.patientProfile.gender) userPrompt += `- Sexe: ${input.patientProfile.gender}\n`;
      if (input.patientProfile.bloodGroup) userPrompt += `- Groupe sanguin: ${input.patientProfile.bloodGroup}\n`;
      if (input.patientProfile.allergies && input.patientProfile.allergies.length > 0) {
        userPrompt += `- Allergies: ${input.patientProfile.allergies.join(', ')}\n`;
      }
      if (input.patientProfile.medicalHistory) {
        userPrompt += `- Ant√©c√©dents m√©dicaux: ${input.patientProfile.medicalHistory}\n`;
      }
    }
  }

  userPrompt += `\nPose maintenant les premi√®res questions de pr√©cision pour affiner ton analyse. Commence par saluer le patient et poser 2-3 questions cl√©s.`;

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: OPENAI_MODEL,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt },
        ],
        temperature: 0.7,
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || 'Je vais analyser vos sympt√¥mes.';
  } catch (error: any) {
    console.error('Error calling OpenAI:', error);
    throw new Error(`Erreur lors de l'analyse des sympt√¥mes: ${error.message}`);
  }
}

/**
 * Generate AI response in precision chat conversation
 */
export async function generateChatResponse(
  conversationHistory: ChatMessage[],
  preAnalysisData: SymptomAnalysisInput
): Promise<string> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key is not configured');
  }

  const systemPrompt = `Tu es un assistant m√©dical IA sp√©cialis√© dans l'anamn√®se (recueil de l'histoire m√©dicale).
Ton r√¥le est de poser des questions cibl√©es pour affiner le diagnostic.

R√®gles CRITIQUES:
- Lis TOUT l'historique de conversation avant de poser une nouvelle question
- N'oublie JAMAIS les r√©ponses d√©j√† donn√©es par le patient
- Ne pose JAMAIS de questions d√©j√† r√©pondues dans l'historique
- Pose des questions sp√©cifiques bas√©es UNIQUEMENT sur les informations manquantes
- Maximum 2-3 questions √† la fois
- Langage clair et accessible
- Sois empathique
- Si tu as assez d'informations, remercie le patient et indique que tu vas proc√©der √† l'analyse finale
- Ne pose JAMAIS de diagnostic d√©finitif dans cette phase
- √âvite absolument la r√©p√©tition - si une information est d√©j√† dans le contexte ou l'historique, ne la redemande pas`;

  // Build conversation messages
  const messages: ChatMessage[] = [
    { role: 'system', content: systemPrompt },
    ...conversationHistory,
  ];

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: OPENAI_MODEL,
        messages: messages,
        temperature: 0.7,
        max_tokens: 400,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || 'Pouvez-vous me donner plus de pr√©cisions ?';
  } catch (error: any) {
    console.error('Error calling OpenAI:', error);
    throw new Error(`Erreur lors de la g√©n√©ration de la r√©ponse: ${error.message}`);
  }
}

/**
 * Generate final AI report with diagnostic hypotheses
 * CRITICAL: Includes images via GPT-4o Vision API and all modalities
 */
export async function generateAIReport(
  preAnalysisData: SymptomAnalysisInput,
  conversationHistory: ChatMessage[],
  imageUrls?: string[] // CRITICAL: Images to analyze with Vision API
): Promise<AIReportData> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key is not configured');
  }

  const systemPrompt = `Tu es HopeVision, IA d‚ÄôANALYSE PR√âLIMINAIRE DES SYMPT√îMES (aide √† la d√©cision, pas un m√©decin).
Pas de diagnostic certain, pas de prescription. Multimodal : texte, voix, images, documents, chat, profil.

Rappels de s√©curit√© :
- Jamais ¬´ vous n‚Äôavez rien ¬ª. Pr√©f√©rer ¬´ situation mod√©r√©e mais avis m√©dical n√©cessaire ¬ª.
- Aucun traitement/posologie.
- Signes de gravit√© (douleur thoracique intense, signes neuro aigus, d√©tresse respi, saignement abondant, perte de connaissance‚Ä¶) => dire clairement :
  ¬´ Appelez imm√©diatement les services d‚Äôurgence (SAMU / num√©ro local). Ne vous fiez pas uniquement √† cette application. ¬ª
- Images : d√©crire prudemment, ¬´ pourrait correspondre √†‚Ä¶ ¬ª, jamais conclure seul.

Format de r√©ponse REQUIS (JSON strict):
{
  "summary": "R√©sum√© clinique synth√©tique (2-3 phrases, en FRAN√áAIS)",
  "explainability_data": {
    "text_analysis": ["Points cl√©s du texte"],
    "voice_analysis": ["Points cl√©s de la voix (essoufflement, toux, pauses respiratoires‚Ä¶)"],
    "image_analysis": ["Observations images (formulation prudente)"],
    "document_analysis": ["Points extraits des documents (PDF, analyses)"],
    "correlation": "Analyse crois√©e entre sources",
    "recommended_actions": ["Action recommand√©e 1", "Action recommand√©e 2", "Action recommand√©e 3"],
    "warning_signs": ["Signe d'alerte 1", "Signe d'alerte 2", "Signe d'alerte 3"]
  },
  "diagnostic_hypotheses": [
    {
      "disease_name": "Nom de la pathologie (hypoth√®se, pas confirm√©)",
      "confidence": nombre entre 0 et 100,
      "severity": "low" | "medium" | "high",
      "keywords": ["mot-cl√©1", "mot-cl√©2", "mot-cl√©3"],
      "explanation": "Justification bas√©e sur TOUTES les sources",
      "is_primary": true/false
    }
  ],
  "overall_severity": "low" | "medium" | "high",
  "overall_confidence": nombre entre 0 et 100,
  "primary_diagnosis": "Hypoth√®se principale",
  "primary_diagnosis_confidence": nombre entre 0 et 100,
  "recommendation_action": "Ex: 'Consultation d'urgence imm√©diate' ou 'Consultation recommand√©e dans les 24-48h'",
  "recommendation_text": "Explication d√©taill√©e de la recommandation"
}

R√®gles importantes:
- 3-5 hypoth√®ses max, ordonn√©es. is_primary=true pour la plus probable.
- Combine TOUTES les sources (texte, voix, images, documents, chat, profil) et corr√®le-les.
- Pas de probabilit√©s num√©riques en dehors des champs demand√©s. Pas de traitement m√©dicamenteux.
- Langage clair, empathique, FRAN√áAIS. Rappeler que seul un professionnel peut confirmer.
- Ne g√©n√®re QUE du JSON valide, sans texte avant ou apr√®s.`;

  const antiHallucinationRules = `
R√àGLES ANTI-HALLUCINATION ET TRA√áABILIT√â DES SYMPT√îMES
- Ne d√©clare un sympt√¥me pr√©sent que s‚Äôil appara√Æt dans le texte patient, une r√©ponse explicite, ou un document sp√©cifique au patient. Poser une question ne signifie pas pr√©sence.
- Distinguer clairement :
  - Sympt√¥mes d√©clar√©s par le patient
  - Sympt√¥mes observ√©s sur images (au conditionnel)
  - Informations issues de documents g√©n√©raux (non sp√©cifiques)
  - Pistes/hypoth√®ses (jamais comme faits)
- Ne jamais inventer ni amplifier : un PDF g√©n√©ral ne prouve pas que le patient a ces sympt√¥mes.
- Utiliser les documents g√©n√©raux comme base de connaissance, pas comme description du patient.
- Dans les rapports d√©taill√©s, ajouter une section ¬´ Origine des informations ¬ª avec ces cat√©gories.
- Si une hypoth√®se repose sur un seul sympt√¥me non sp√©cifique sans √©l√©ments concordants, la qualifier de ¬´ hypoth√®se tr√®s incertaine ¬ª ou l‚Äôomettre si elle ajoute de la confusion.
`;

  const systemPromptFull = `${systemPrompt}\n${antiHallucinationRules}`;

  // CRITICAL: Build structured multimodal medical prompt
  // Extract document contents if available (from unifiedContext or preAnalysisData)
  const documentContents = preAnalysisData.unifiedContext?.document_contents || 
    (preAnalysisData as any).documentContents || [];

  // Build structured user prompt following the medical template
  let userPrompt = `üè• PROMPT : Analyse Multimodale de Diagnostic M√©dical ‚Äì HOPEVISIONAI

===========================
DONN√âES DU PATIENT
===========================
Profil :
- √Çge : ${preAnalysisData.patientProfile?.age || 'Non sp√©cifi√©'}
- Sexe : ${preAnalysisData.patientProfile?.gender || 'Non sp√©cifi√©'}
- Ant√©c√©dents m√©dicaux : ${preAnalysisData.patientProfile?.medicalHistory || 'Aucun ant√©c√©dent mentionn√©'}
- Allergies : ${preAnalysisData.patientProfile?.allergies?.join(', ') || 'Aucune allergie mentionn√©e'}
- Traitements actuels : ${(preAnalysisData.patientProfile as any)?.currentTreatments || 'Aucun traitement mentionn√©'}
- Mode de vie : ${(preAnalysisData.patientProfile as any)?.lifestyle || 'Non sp√©cifi√©'}

===========================
SYMPT√îMES COMMUNIQU√âS
===========================
Texte √©crit :
${preAnalysisData.textInput || 'Aucun texte fourni'}

Transcription vocale :
${preAnalysisData.voiceTranscript || 'Aucune transcription vocale'}

Analyse vocale (optionnel) :
${preAnalysisData.voiceTranscript ? '√Ä analyser : essoufflement, toux, pauses respiratoires, fatigue vocale' : 'Non disponible'}

Tags/Pr√©cisions rapides :
${preAnalysisData.selectedChips && preAnalysisData.selectedChips.length > 0 
  ? preAnalysisData.selectedChips.join(', ') 
  : 'Aucun tag s√©lectionn√©'}

===========================
DONN√âES VISUELLES
===========================
Images m√©dicales analys√©es :
${imageUrls && imageUrls.length > 0 
  ? `${imageUrls.length} image(s) fournie(s) - Analyse visuelle jointe au message pour analyse d√©taill√©e`
  : 'Aucune image fournie'}

${preAnalysisData.unifiedContext?.image_analyses && preAnalysisData.unifiedContext.image_analyses.length > 0
  ? `Analyses pr√©liminaires des images:\n${preAnalysisData.unifiedContext.image_analyses.join('\n')}`
  : ''}

===========================
DOCUMENTS M√âDICAUX UPLOAD√âS
===========================
Analyses sanguines / rapports :
${Array.isArray(documentContents) && documentContents.length > 0
  ? documentContents.map((doc, idx) => `Document ${idx + 1}:\n${doc}`).join('\n\n---\n\n')
  : (preAnalysisData.documentUrls && preAnalysisData.documentUrls.length > 0
      ? `${preAnalysisData.documentUrls.length} document(s) fourni(s) mais contenu non extrait`
      : 'Aucun document fourni')}

===========================
QUESTIONS DE PR√âCISION (Q&A)
===========================
Historique de conversation :
${conversationHistory.length > 0
  ? conversationHistory
      .filter(msg => msg.role !== 'system')
      .map(msg => `${msg.role === 'user' ? 'Patient' : 'IA'}: ${msg.content}`)
      .join('\n\n')
  : 'Aucune conversation de pr√©cision'}

${preAnalysisData.chatAnswers
  ? `R√©ponses du patient r√©sum√©es:\n${preAnalysisData.chatAnswers}`
  : ''}

===========================
OBJECTIF
===========================
√Ä partir de toutes ces donn√©es combin√©es, g√©n√®re un rapport m√©dical professionnel selon la structure suivante :

1. **R√©sum√© clinique**
   Synth√®se rapide et claire de ce que pr√©sente le patient.

2. **Analyse multimodale unifi√©e**
   - Analyse des sympt√¥mes textuels
   - Analyse de la voix (respiration, fatigue vocale, toux‚Ä¶)
   - Analyse des images (l√©sions, anomalies visibles)
   - Analyse des documents m√©dicaux
   - Corr√©lation entre les diff√©rentes sources
   - Prise en compte de l'√¢ge + ant√©c√©dents + allergies + traitements

3. **Hypoth√®ses diagnostiques probables**
   Pour chaque hypoth√®se :
   - nom de la pathologie
   - justification clinique bas√©e sur TOUTES les sources
   - niveau de confiance (%)
   - niveau de gravit√©

4. **Niveau de gravit√©** (faible / mod√©r√© / √©lev√©)
   Justifie ce niveau en utilisant toutes les donn√©es.
   Note: Pour les urgences vitales, utilise "high" (la base de donn√©es n'accepte que "low", "medium", "high").

5. **Recommandations m√©dicales**
   - examens compl√©mentaires utiles
   - consultations recommand√©es
   - traitement d'attente (non m√©dicalis√©)
   - signaux d'alerte √† surveiller

6. **Conclusion m√©dicale**
   Bas√©e sur toutes les sources, comme un m√©decin qui synth√©tise.

IMPORTANT :
- Utilise un langage m√©dical clair et compr√©hensible pour un patient.
- Ne donne jamais de diagnostic d√©finitif : fournir des hypoth√®ses.
- Ne proposer aucun m√©dicament (conformit√© RGPD/HDS).
- Toujours recommander un professionnel en cas de doute ou gravit√©.
- Combine TOUTES les sources dans un seul raisonnement clinique.
- Corr√®le les donn√©es entre elles (ex: "L'essoufflement dans la voix corrobore l'image montrant X").

G√©n√®re maintenant le rapport complet au format JSON strict.`;

  // CRITICAL: Build messages array with images if available (GPT-4o Vision)
  const messages: any[] = [
    { role: 'system', content: systemPromptFull },
  ];

  // CRITICAL: Log the complete prompt for debugging
  console.log(`[OpenAI] üìù PROMPT COMPLET POUR RAPPORT:`);
  console.log(`[OpenAI] System Prompt (${systemPromptFull.length} chars):`, systemPromptFull.substring(0, 200) + '...');
  console.log(`[OpenAI] User Prompt (${userPrompt.length} chars):`, userPrompt.substring(0, 500) + '...');
  console.log(`[OpenAI] Context includes:`, {
    textInput: !!preAnalysisData.textInput,
    voiceTranscript: !!preAnalysisData.voiceTranscript,
    selectedChips: preAnalysisData.selectedChips?.length || 0,
    imageUrls: imageUrls?.length || 0,
    documentUrls: preAnalysisData.documentUrls?.length || 0,
    chatMessages: conversationHistory.length,
    patientProfile: !!preAnalysisData.patientProfile,
    unifiedContext: !!preAnalysisData.unifiedContext,
  });

  // CRITICAL: If images are available, use Vision API format
  if (imageUrls && imageUrls.length > 0 && preAnalysisData.unifiedContext?.image_urls) {
    console.log(`[OpenAI] Using Vision API for ${imageUrls.length} image(s)`);
    
    // Build content array with text and images
    const contentArray: any[] = [
      { type: 'text', text: userPrompt }
    ];

    // Add all images to the content array
    for (const imageUrl of imageUrls) {
      try {
        // CRITICAL: Validate that URL is an image before processing
        const urlLower = imageUrl.toLowerCase();
        const isImageUrl = urlLower.match(/\.(jpg|jpeg|png|gif|webp|bmp)(\?|$)/i);
        
        if (!isImageUrl) {
          console.warn(`[OpenAI] Skipping non-image URL: ${imageUrl}`);
          continue;
        }

        // CRITICAL: Download image using Supabase Storage API if needed
        // This handles both public and private URLs
        let imageBlob: Blob;
        try {
          const { downloadImageFromStorage } = await import('../utils/imageDownload');
          imageBlob = await downloadImageFromStorage(imageUrl);
          console.log(`[OpenAI] ‚úÖ Image downloaded successfully: ${imageUrl.substring(0, 50)}...`);
        } catch (downloadError: any) {
          console.error(`[OpenAI] ‚ùå Failed to download image ${imageUrl}:`, downloadError);
          // Continue with next image instead of failing entire request
          continue;
        }
        
        // CRITICAL: Validate and correct MIME type
        let mimeType = imageBlob.type;
        
        if (!mimeType || !mimeType.startsWith('image/')) {
          // Try to detect MIME type from URL extension
          if (urlLower.includes('.png')) {
            mimeType = 'image/png';
          } else if (urlLower.includes('.gif')) {
            mimeType = 'image/gif';
          } else if (urlLower.includes('.webp')) {
            mimeType = 'image/webp';
          } else if (urlLower.includes('.bmp')) {
            mimeType = 'image/bmp';
          } else if (urlLower.includes('.jpg') || urlLower.includes('.jpeg')) {
            mimeType = 'image/jpeg';
          } else {
            mimeType = 'image/jpeg'; // Default fallback
          }
          
          console.warn(`[OpenAI] Blob type is "${imageBlob.type}", using detected type "${mimeType}" for ${imageUrl}`);
        }
        
        // CRITICAL: Ensure mimeType is valid for Vision API
        const validMimeType = mimeType && mimeType.startsWith('image/') 
          ? mimeType 
          : 'image/jpeg'; // Fallback to jpeg if still invalid
        
        const base64Image = await blobToBase64(imageBlob);
        
        contentArray.push({
          type: 'image_url',
          image_url: {
            url: `data:${validMimeType};base64,${base64Image}`
          }
        });
      } catch (imageError: any) {
        console.error(`[OpenAI] Error processing image ${imageUrl}:`, imageError);
        // Continue with other images - don't fail the entire request
      }
    }

    messages.push({
      role: 'user',
      content: contentArray
    });
  } else {
    // No images, use text-only format
    messages.push({
      role: 'user',
      content: userPrompt
    });
  }

  // CRITICAL: Log the complete prompt before sending to OpenAI
  console.log('üìù ========== FINAL OPENAI PROMPT START (RAPPORT) ==========');
  console.log('üìù Full request payload:');
  
  // CRITICAL: Check if messages contain images
  const hasImages = messages.some((msg: any) => 
    Array.isArray(msg.content) && msg.content.some((item: any) => item.type === 'image_url')
  );
  
  const requestPayload: any = {
    model: 'gpt-4o',
    messages: messages,
    temperature: 0.3,
    max_tokens: 4000, // CRITICAL: Increased for comprehensive medical reports
  };
  
  // CRITICAL: response_format: json_object can cause empty responses when combined with images
  // Only use it when there are no images
  if (!hasImages) {
    requestPayload.response_format = { type: 'json_object' };
    console.log('[OpenAI] ‚úÖ Using response_format: json_object (no images)');
  } else {
    console.log('[OpenAI] ‚ö†Ô∏è NOT using response_format: json_object (images present - forcing JSON in prompt)');
    // The system prompt already instructs to return JSON, so it should work without response_format
  }
  
  console.log(JSON.stringify(requestPayload, null, 2));
  console.log('üìù Messages breakdown:');
  messages.forEach((msg, idx) => {
    console.log(`üìù Message ${idx + 1} (${msg.role}):`);
    if (Array.isArray(msg.content)) {
      // Vision API format - log each content item
      msg.content.forEach((item: any, itemIdx: number) => {
        if (item.type === 'text') {
          console.log(`üìù   Text content (${item.text.length} chars): ${item.text.substring(0, 200)}...`);
        } else if (item.type === 'image_url') {
          const base64Preview = item.image_url.url.substring(0, 100);
          console.log(`üìù   Image ${itemIdx}: ${base64Preview}... (${item.image_url.url.length} chars total)`);
        }
      });
    } else {
      console.log(`üìù   Content (${msg.content.length} chars): ${msg.content.substring(0, 200)}...`);
    }
  });
  console.log('üìù ========== FINAL OPENAI PROMPT END (RAPPORT) ==========');

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify(requestPayload),
    });

    if (!response.ok) {
      const errorText = await response.text();
      let errorData;
      try {
        errorData = JSON.parse(errorText);
      } catch {
        errorData = { message: errorText };
      }
      console.error('[OpenAI] ‚ùå API Error Response:', {
        status: response.status,
        statusText: response.statusText,
        error: errorData
      });
      throw new Error(`OpenAI API error: ${errorData.error?.message || errorData.message || response.statusText}`);
    }

    const data = await response.json();
    
    // CRITICAL: Log full response for debugging
    console.log('[OpenAI] üì• Full API Response:', JSON.stringify(data, null, 2));
    console.log('[OpenAI] üìä Response Summary:', {
      hasChoices: !!data.choices,
      choicesLength: data.choices?.length || 0,
      firstChoice: data.choices?.[0] ? {
        hasMessage: !!data.choices[0].message,
        hasContent: !!data.choices[0].message?.content,
        contentLength: data.choices[0].message?.content?.length || 0,
        finishReason: data.choices[0].finish_reason,
        contentPreview: data.choices[0].message?.content?.substring(0, 200) || 'NO CONTENT'
      } : null,
      usage: data.usage
    });

    const content = data.choices?.[0]?.message?.content;

    if (!content) {
      console.error('[OpenAI] ‚ùå Empty content in response - Full response data:');
      console.error(JSON.stringify(data, null, 2));
      
      // Check if response was cut off due to token limit
      if (data.choices?.[0]?.finish_reason === 'length') {
        throw new Error('Rapport trop long - r√©ponse tronqu√©e par OpenAI. Augmentez max_tokens.');
      }
      
      // If finish_reason is 'stop' but content is empty, it might be a JSON parsing issue
      if (data.choices?.[0]?.finish_reason === 'stop' && !content) {
        console.error('[OpenAI] ‚ö†Ô∏è Finish reason is "stop" but content is empty - possible JSON format issue with images');
        // Try to get any content from the response
        const rawContent = data.choices?.[0]?.message?.content;
        if (rawContent) {
          console.log('[OpenAI] Found raw content:', rawContent.substring(0, 500));
          content = rawContent;
        } else {
          throw new Error('Empty response from OpenAI. Finish reason: stop but no content. Possible issue with JSON format and images.');
        }
      } else {
        throw new Error(`Empty response from OpenAI. Finish reason: ${data.choices?.[0]?.finish_reason || 'unknown'}`);
      }
    }

    // Parse JSON response
    let reportData: AIReportData;
    try {
      reportData = JSON.parse(content);
    } catch (parseError) {
      // Try to extract JSON from markdown code blocks if present
      const jsonMatch = content.match(/```(?:json)?\s*(\{[\s\S]*\})\s*```/);
      if (jsonMatch) {
        reportData = JSON.parse(jsonMatch[1]);
      } else {
        throw new Error('Invalid JSON response from AI');
      }
    }

    // CRITICAL: Validate and normalize overall_severity to match database constraint
    // Database only accepts: 'low' | 'medium' | 'high' (NOT 'critical')
    if (reportData.overall_severity) {
      const severity = reportData.overall_severity.toLowerCase();
      if (severity === 'critical') {
        console.warn('[OpenAI] ‚ö†Ô∏è Converting "critical" severity to "high" to match database constraint');
        reportData.overall_severity = 'high';
      } else if (!['low', 'medium', 'high'].includes(severity)) {
        console.warn(`[OpenAI] ‚ö†Ô∏è Invalid severity "${severity}", defaulting to "medium"`);
        reportData.overall_severity = 'medium';
      } else {
        reportData.overall_severity = severity as 'low' | 'medium' | 'high';
      }
    }

    // Validate required fields
    if (!reportData.diagnostic_hypotheses || !Array.isArray(reportData.diagnostic_hypotheses)) {
      throw new Error('Missing diagnostic_hypotheses in AI response');
    }

    // Ensure at least one primary diagnosis
    if (!reportData.diagnostic_hypotheses.some((h: DiagnosticHypothesis) => h.is_primary)) {
      if (reportData.diagnostic_hypotheses.length > 0) {
        reportData.diagnostic_hypotheses[0].is_primary = true;
      }
    }

    // Ensure explainability_data exists
    if (!reportData.explainability_data) {
      reportData.explainability_data = {
        text_analysis: reportData.diagnostic_hypotheses.map((h: DiagnosticHypothesis) => ({
          label: h.disease_name,
          description: h.explanation
        }))
      };
    }

    return reportData;
  } catch (error: any) {
    console.error('Error generating AI report:', error);
    throw new Error(`Erreur lors de la g√©n√©ration du rapport: ${error.message}`);
  }
}

/**
 * Transcribe audio using OpenAI Whisper API
 */
export async function transcribeAudio(audioFile: File): Promise<string> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key is not configured');
  }

  const formData = new FormData();
  formData.append('file', audioFile);
  formData.append('model', 'whisper-1');
  formData.append('language', 'fr');

  try {
    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: formData,
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return data.text || '';
  } catch (error: any) {
    console.error('Error transcribing audio:', error);
    throw new Error(`Erreur lors de la transcription: ${error.message}`);
  }
}

/**
 * Analyze images using OpenAI Vision API
 */
export async function analyzeImage(imageUrl: string, context?: string): Promise<string> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key is not configured');
  }

  console.log(`[OpenAI Vision] üîç D√©but analyse d'image`);
  console.log(`[OpenAI Vision] üì∑ URL: ${imageUrl.substring(0, 80)}...`);
  console.log(`[OpenAI Vision] üìù Contexte: ${context || 'Aucun contexte sp√©cifique'}`);

  // CRITICAL: Validate that URL is an image before processing
  const urlLower = imageUrl.toLowerCase();
  const isImageUrl = urlLower.match(/\.(jpg|jpeg|png|gif|webp|bmp)(\?|$)/i);
  
  if (!isImageUrl) {
    console.error(`[OpenAI Vision] ‚ùå URL n'est pas une image: ${imageUrl}`);
    throw new Error(`URL is not an image: ${imageUrl}`);
  }

  // CRITICAL: Download image using Supabase Storage API if needed
  let imageBlob: Blob;
  try {
    const { downloadImageFromStorage } = await import('../utils/imageDownload');
    imageBlob = await downloadImageFromStorage(imageUrl);
    console.log(`[OpenAI Vision] ‚úÖ Image t√©l√©charg√©e avec succ√®s (${imageBlob.size} bytes, type: ${imageBlob.type})`);
  } catch (downloadError: any) {
    console.error(`[OpenAI Vision] ‚ùå √âchec du t√©l√©chargement de l'image:`, downloadError);
    throw new Error(`Failed to download image: ${downloadError.message}`);
  }
  
  // CRITICAL: Validate and correct MIME type if needed
  let mimeType = imageBlob.type;
  
  if (!mimeType || !mimeType.startsWith('image/')) {
    // Try to detect MIME type from URL extension
    if (urlLower.includes('.png')) {
      mimeType = 'image/png';
    } else if (urlLower.includes('.gif')) {
      mimeType = 'image/gif';
    } else if (urlLower.includes('.webp')) {
      mimeType = 'image/webp';
    } else if (urlLower.includes('.bmp')) {
      mimeType = 'image/bmp';
    } else {
      mimeType = 'image/jpeg'; // Default fallback
    }
    
    console.warn(`[OpenAI] Blob type is "${imageBlob.type}", using detected type "${mimeType}" for ${imageUrl}`);
  }
  
  const base64Image = await blobToBase64(imageBlob);

  // CRITICAL: Ensure mimeType is valid for Vision API
  const validMimeType = mimeType && mimeType.startsWith('image/') 
    ? mimeType 
    : 'image/jpeg'; // Fallback to jpeg if still invalid

  const systemPrompt = `Tu es un assistant m√©dical IA expert en analyse d'images m√©dicales.
Analyse l'image fournie et d√©cris ce que tu observes de mani√®re m√©dicale pr√©cise.
Note les √©l√©ments visuels pertinents pour l'analyse des sympt√¥mes.`;

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'gpt-4o', // Use vision-capable model
        messages: [
          { role: 'system', content: systemPrompt },
          {
            role: 'user',
            content: [
              { type: 'text', text: context || 'Analyse cette image m√©dicale et d√©cris ce que tu observes.' },
              { type: 'image_url', image_url: { url: `data:${validMimeType};base64,${base64Image}` } },
            ],
          },
        ],
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      console.error(`[OpenAI Vision] ‚ùå Erreur API:`, error);
      throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    const analysisResult = data.choices[0]?.message?.content || 'Image analys√©e.';
    
    // CRITICAL: Log the complete analysis result
    console.log(`[OpenAI Vision] ‚úÖ Analyse compl√©t√©e avec succ√®s`);
    console.log(`[OpenAI Vision] üìä R√©sultat de l'analyse (${analysisResult.length} caract√®res):`);
    console.log(`[OpenAI Vision] ========== ANALYSE IMAGE PAR IA ==========`);
    console.log(analysisResult);
    console.log(`[OpenAI Vision] ==========================================`);
    console.log(`[OpenAI Vision] üìà Usage tokens:`, data.usage);
    
    return analysisResult;
  } catch (error: any) {
    console.error(`[OpenAI Vision] ‚ùå Erreur lors de l'analyse de l'image:`, error);
    console.error(`[OpenAI Vision] D√©tails de l'erreur:`, {
      message: error.message,
      stack: error.stack,
      url: imageUrl.substring(0, 80)
    });
    throw new Error(`Erreur lors de l'analyse de l'image: ${error.message}`);
  }
}

/**
 * Helper function to convert blob to base64
 */
function blobToBase64(blob: Blob): Promise<string> {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = (reader.result as string).split(',')[1];
      resolve(base64String);
    };
    reader.onerror = reject;
    reader.readAsDataURL(blob);
  });
}

